{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/snehachoudhary/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import threading # will potentially use multi-threading\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Computer Terminal Systems Inc said\\nit has completed the sale of 200,000 shares of its common\\nstock, and warrants to acquire an additional one mln shares, to\\n<Sedio N.V.> of Lugano, Switzerland for 50,000 dlrs.\\n    The company said the warrants are exercisable for five\\nyears at a purchase price of .125 dlrs per share.\\n    Computer Terminal said Sedio also has the right to buy\\nadditional shares and increase its total holdings up to 40 pct\\nof the Computer Terminal's outstanding common stock under\\ncertain circumstances involving change of control at the\\ncompany.\\n    The company said if the conditions occur the warrants would\\nbe exercisable at a price equal to 75 pct of its common stock's\\nmarket price at the time, not to exceed 1.50 dlrs per share.\\n    Computer Terminal also said it sold the technolgy rights to\\nits Dot Matrix impact technology, including any future\\nimprovements, to <Woodco Inc> of Houston, Tex. for 200,000\\ndlrs. But, it said it would continue to be the exclusive\\nworldwide licensee of the technology for Woodco.\\n    The company said the moves were part of its reorganization\\nplan and would help pay current operation costs and ensure\\nproduct delivery.\\n    Computer Terminal makes computer generated labels, forms,\\ntags and ticket printers and terminals.\\n Reuter\\n\\x03\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open(\"DATA2/reut2-000.sgm\", 'r')\n",
    "#print(\"file\",file)\n",
    "text = data.read()\n",
    "#print(text)\n",
    "data.close()\n",
    "data_beautiful = BeautifulSoup(text, \"html.parser\")\n",
    "#print(data_beautiful)\n",
    "for reuter in data_beautiful.find_all(\"reuters\"):\n",
    "    text1 = reuter.find('text')\n",
    "    title = text1.title\n",
    "    body = text1.body\n",
    "    if body!=None:\n",
    "        body_len = len(body.text.split())\n",
    "        if (body_len> 200) & (body_len <400):\n",
    "            print(\"-------------------------------------------------------------\")\n",
    "            print(body_len)\n",
    "            break\n",
    "body.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Computer Terminal Systems Inc said\n",
      "it has completed the sale of 200,000 shares of its common\n",
      "stock, and warrants to acquire an additional one mln shares, to\n",
      "<Sedio N.V.> of Lugano, Switzerland for 50,000 dlrs.\n",
      "    The company said the warrants are exercisable for five\n",
      "years at a purchase price of .125 dlrs per share.\n",
      "    Computer Terminal said Sedio also has the right to buy\n",
      "additional shares and increase its total holdings up to 40 pct\n",
      "of the Computer Terminal's outstanding common stock under\n",
      "certain circumstances involving change of control at the\n",
      "company.\n",
      "    The company said if the conditions occur the warrants would\n",
      "be exercisable at a price equal to 75 pct of its common stock's\n",
      "market price at the time, not to exceed 1.50 dlrs per share.\n",
      "    Computer Terminal also said it sold the technolgy rights to\n",
      "its Dot Matrix impact technology, including any future\n",
      "improvements, to <Woodco Inc> of Houston, Tex. for 200,000\n",
      "dlrs. But, it said it would continue to be the exclusive\n",
      "worldwide licensee of the technology for Woodco.\n",
      "    The company said the moves were part of its reorganization\n",
      "plan and would help pay current operation costs and ensure\n",
      "product delivery.\n",
      "    Computer Terminal makes computer generated labels, forms,\n",
      "tags and ticket printers and terminals.\n",
      " Reuter\n",
      "\u0003 [SEP]\n"
     ]
    }
   ],
   "source": [
    "marked_text = \"[CLS] \" + body.text + \" [SEP]\"\n",
    "print (marked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'computer', 'terminal', 'systems', 'inc', 'said', 'it', 'has', 'completed', 'the', 'sale', 'of', '200', ',', '000', 'shares', 'of', 'its', 'common', 'stock', ',', 'and', 'warrant', '##s', 'to', 'acquire', 'an', 'additional', 'one', 'ml', '##n', 'shares', ',', 'to', '<', 'se', '##dio', 'n', '.', 'v', '.', '>', 'of', 'lu', '##gano', ',', 'switzerland', 'for', '50', ',', '000', 'dl', '##rs', '.', 'the', 'company', 'said', 'the', 'warrant', '##s', 'are', 'ex', '##er', '##cis', '##able', 'for', 'five', 'years', 'at', 'a', 'purchase', 'price', 'of', '.', '125', 'dl', '##rs', 'per', 'share', '.', 'computer', 'terminal', 'said', 'se', '##dio', 'also', 'has', 'the', 'right', 'to', 'buy', 'additional', 'shares', 'and', 'increase', 'its', 'total', 'holdings', 'up', 'to', '40', 'pc', '##t', 'of', 'the', 'computer', 'terminal', \"'\", 's', 'outstanding', 'common', 'stock', 'under', 'certain', 'circumstances', 'involving', 'change', 'of', 'control', 'at', 'the', 'company', '.', 'the', 'company', 'said', 'if', 'the', 'conditions', 'occur', 'the', 'warrant', '##s', 'would', 'be', 'ex', '##er', '##cis', '##able', 'at', 'a', 'price', 'equal', 'to', '75', 'pc', '##t', 'of', 'its', 'common', 'stock', \"'\", 's', 'market', 'price', 'at', 'the', 'time', ',', 'not', 'to', 'exceed', '1', '.', '50', 'dl', '##rs', 'per', 'share', '.', 'computer', 'terminal', 'also', 'said', 'it', 'sold', 'the', 'techno', '##l', '##gy', 'rights', 'to', 'its', 'dot', 'matrix', 'impact', 'technology', ',', 'including', 'any', 'future', 'improvements', ',', 'to', '<', 'wood', '##co', 'inc', '>', 'of', 'houston', ',', 'tex', '.', 'for', '200', ',', '000', 'dl', '##rs', '.', 'but', ',', 'it', 'said', 'it', 'would', 'continue', 'to', 'be', 'the', 'exclusive', 'worldwide', 'license', '##e', 'of', 'the', 'technology', 'for', 'wood', '##co', '.', 'the', 'company', 'said', 'the', 'moves', 'were', 'part', 'of', 'its', 'reorganization', 'plan', 'and', 'would', 'help', 'pay', 'current', 'operation', 'costs', 'and', 'ensure', 'product', 'delivery', '.', 'computer', 'terminal', 'makes', 'computer', 'generated', 'labels', ',', 'forms', ',', 'tags', 'and', 'ticket', 'printers', 'and', 'terminals', '.', 're', '##uter', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 101)\n",
      "('computer', 3274)\n",
      "('terminal', 5536)\n",
      "('systems', 3001)\n",
      "('inc', 4297)\n",
      "('said', 2056)\n",
      "('it', 2009)\n",
      "('has', 2038)\n",
      "('completed', 2949)\n",
      "('the', 1996)\n",
      "('sale', 5096)\n",
      "('of', 1997)\n",
      "('200', 3263)\n",
      "(',', 1010)\n",
      "('000', 2199)\n",
      "('shares', 6661)\n",
      "('of', 1997)\n",
      "('its', 2049)\n",
      "('common', 2691)\n",
      "('stock', 4518)\n",
      "(',', 1010)\n",
      "('and', 1998)\n",
      "('warrant', 10943)\n",
      "('##s', 2015)\n",
      "('to', 2000)\n",
      "('acquire', 9878)\n",
      "('an', 2019)\n",
      "('additional', 3176)\n",
      "('one', 2028)\n",
      "('ml', 19875)\n",
      "('##n', 2078)\n",
      "('shares', 6661)\n",
      "(',', 1010)\n",
      "('to', 2000)\n",
      "('<', 1026)\n",
      "('se', 7367)\n",
      "('##dio', 20617)\n",
      "('n', 1050)\n",
      "('.', 1012)\n",
      "('v', 1058)\n",
      "('.', 1012)\n",
      "('>', 1028)\n",
      "('of', 1997)\n",
      "('lu', 11320)\n",
      "('##gano', 29451)\n",
      "(',', 1010)\n",
      "('switzerland', 5288)\n",
      "('for', 2005)\n",
      "('50', 2753)\n",
      "(',', 1010)\n",
      "('000', 2199)\n",
      "('dl', 21469)\n",
      "('##rs', 2869)\n",
      "('.', 1012)\n",
      "('the', 1996)\n",
      "('company', 2194)\n",
      "('said', 2056)\n",
      "('the', 1996)\n",
      "('warrant', 10943)\n",
      "('##s', 2015)\n",
      "('are', 2024)\n",
      "('ex', 4654)\n",
      "('##er', 2121)\n",
      "('##cis', 22987)\n",
      "('##able', 3085)\n",
      "('for', 2005)\n",
      "('five', 2274)\n",
      "('years', 2086)\n",
      "('at', 2012)\n",
      "('a', 1037)\n",
      "('purchase', 5309)\n",
      "('price', 3976)\n",
      "('of', 1997)\n",
      "('.', 1012)\n",
      "('125', 8732)\n",
      "('dl', 21469)\n",
      "('##rs', 2869)\n",
      "('per', 2566)\n",
      "('share', 3745)\n",
      "('.', 1012)\n",
      "('computer', 3274)\n",
      "('terminal', 5536)\n",
      "('said', 2056)\n",
      "('se', 7367)\n",
      "('##dio', 20617)\n",
      "('also', 2036)\n",
      "('has', 2038)\n",
      "('the', 1996)\n",
      "('right', 2157)\n",
      "('to', 2000)\n",
      "('buy', 4965)\n",
      "('additional', 3176)\n",
      "('shares', 6661)\n",
      "('and', 1998)\n",
      "('increase', 3623)\n",
      "('its', 2049)\n",
      "('total', 2561)\n",
      "('holdings', 9583)\n",
      "('up', 2039)\n",
      "('to', 2000)\n",
      "('40', 2871)\n",
      "('pc', 7473)\n",
      "('##t', 2102)\n",
      "('of', 1997)\n",
      "('the', 1996)\n",
      "('computer', 3274)\n",
      "('terminal', 5536)\n",
      "(\"'\", 1005)\n",
      "('s', 1055)\n",
      "('outstanding', 5151)\n",
      "('common', 2691)\n",
      "('stock', 4518)\n",
      "('under', 2104)\n",
      "('certain', 3056)\n",
      "('circumstances', 6214)\n",
      "('involving', 5994)\n",
      "('change', 2689)\n",
      "('of', 1997)\n",
      "('control', 2491)\n",
      "('at', 2012)\n",
      "('the', 1996)\n",
      "('company', 2194)\n",
      "('.', 1012)\n",
      "('the', 1996)\n",
      "('company', 2194)\n",
      "('said', 2056)\n",
      "('if', 2065)\n",
      "('the', 1996)\n",
      "('conditions', 3785)\n",
      "('occur', 5258)\n",
      "('the', 1996)\n",
      "('warrant', 10943)\n",
      "('##s', 2015)\n",
      "('would', 2052)\n",
      "('be', 2022)\n",
      "('ex', 4654)\n",
      "('##er', 2121)\n",
      "('##cis', 22987)\n",
      "('##able', 3085)\n",
      "('at', 2012)\n",
      "('a', 1037)\n",
      "('price', 3976)\n",
      "('equal', 5020)\n",
      "('to', 2000)\n",
      "('75', 4293)\n",
      "('pc', 7473)\n",
      "('##t', 2102)\n",
      "('of', 1997)\n",
      "('its', 2049)\n",
      "('common', 2691)\n",
      "('stock', 4518)\n",
      "(\"'\", 1005)\n",
      "('s', 1055)\n",
      "('market', 3006)\n",
      "('price', 3976)\n",
      "('at', 2012)\n",
      "('the', 1996)\n",
      "('time', 2051)\n",
      "(',', 1010)\n",
      "('not', 2025)\n",
      "('to', 2000)\n",
      "('exceed', 13467)\n",
      "('1', 1015)\n",
      "('.', 1012)\n",
      "('50', 2753)\n",
      "('dl', 21469)\n",
      "('##rs', 2869)\n",
      "('per', 2566)\n",
      "('share', 3745)\n",
      "('.', 1012)\n",
      "('computer', 3274)\n",
      "('terminal', 5536)\n",
      "('also', 2036)\n",
      "('said', 2056)\n",
      "('it', 2009)\n",
      "('sold', 2853)\n",
      "('the', 1996)\n",
      "('techno', 21416)\n",
      "('##l', 2140)\n",
      "('##gy', 6292)\n",
      "('rights', 2916)\n",
      "('to', 2000)\n",
      "('its', 2049)\n",
      "('dot', 11089)\n",
      "('matrix', 8185)\n",
      "('impact', 4254)\n",
      "('technology', 2974)\n",
      "(',', 1010)\n",
      "('including', 2164)\n",
      "('any', 2151)\n",
      "('future', 2925)\n",
      "('improvements', 8377)\n",
      "(',', 1010)\n",
      "('to', 2000)\n",
      "('<', 1026)\n",
      "('wood', 3536)\n",
      "('##co', 3597)\n",
      "('inc', 4297)\n",
      "('>', 1028)\n",
      "('of', 1997)\n",
      "('houston', 5395)\n",
      "(',', 1010)\n",
      "('tex', 16060)\n",
      "('.', 1012)\n",
      "('for', 2005)\n",
      "('200', 3263)\n",
      "(',', 1010)\n",
      "('000', 2199)\n",
      "('dl', 21469)\n",
      "('##rs', 2869)\n",
      "('.', 1012)\n",
      "('but', 2021)\n",
      "(',', 1010)\n",
      "('it', 2009)\n",
      "('said', 2056)\n",
      "('it', 2009)\n",
      "('would', 2052)\n",
      "('continue', 3613)\n",
      "('to', 2000)\n",
      "('be', 2022)\n",
      "('the', 1996)\n",
      "('exclusive', 7262)\n",
      "('worldwide', 4969)\n",
      "('license', 6105)\n",
      "('##e', 2063)\n",
      "('of', 1997)\n",
      "('the', 1996)\n",
      "('technology', 2974)\n",
      "('for', 2005)\n",
      "('wood', 3536)\n",
      "('##co', 3597)\n",
      "('.', 1012)\n",
      "('the', 1996)\n",
      "('company', 2194)\n",
      "('said', 2056)\n",
      "('the', 1996)\n",
      "('moves', 5829)\n",
      "('were', 2020)\n",
      "('part', 2112)\n",
      "('of', 1997)\n",
      "('its', 2049)\n",
      "('reorganization', 17118)\n",
      "('plan', 2933)\n",
      "('and', 1998)\n",
      "('would', 2052)\n",
      "('help', 2393)\n",
      "('pay', 3477)\n",
      "('current', 2783)\n",
      "('operation', 3169)\n",
      "('costs', 5366)\n",
      "('and', 1998)\n",
      "('ensure', 5676)\n",
      "('product', 4031)\n",
      "('delivery', 6959)\n",
      "('.', 1012)\n",
      "('computer', 3274)\n",
      "('terminal', 5536)\n",
      "('makes', 3084)\n",
      "('computer', 3274)\n",
      "('generated', 7013)\n",
      "('labels', 10873)\n",
      "(',', 1010)\n",
      "('forms', 3596)\n",
      "(',', 1010)\n",
      "('tags', 22073)\n",
      "('and', 1998)\n",
      "('ticket', 7281)\n",
      "('printers', 23557)\n",
      "('and', 1998)\n",
      "('terminals', 17703)\n",
      "('.', 1012)\n",
      "('re', 2128)\n",
      "('##uter', 19901)\n",
      "('[SEP]', 102)\n"
     ]
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "  print (tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])  #A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n",
      "Number of batches: 1\n",
      "Number of tokens: 274\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers)) #No of hidden layers in the model\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i])) #No of sentences\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i])) #No of tokens in our sentence\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))#No of features/hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 274\n",
      "Number of layers per token: 12\n"
     ]
    }
   ],
   "source": [
    "# Convert the hidden state embeddings into single token vectors\n",
    "\n",
    "# Holds the list of 12 layer embeddings for each token\n",
    "# Will have the shape: [# tokens, # layers, # features]\n",
    "token_embeddings = [] \n",
    "\n",
    "# For each token in the sentence...\n",
    "for token_i in range(len(tokenized_text)):\n",
    "  \n",
    "  # Holds 12 layers of hidden states for each token \n",
    "  hidden_layers = [] \n",
    "  \n",
    "  # For each of the 12 layers...\n",
    "  for layer_i in range(len(encoded_layers)):\n",
    "    \n",
    "    # Lookup the vector for `token_i` in `layer_i`\n",
    "    vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "    \n",
    "    hidden_layers.append(vec)\n",
    "    \n",
    "  token_embeddings.append(hidden_layers)\n",
    "\n",
    "# Sanity check the dimensions:\n",
    "print (\"Number of tokens in sequence:\", len(token_embeddings))\n",
    "print (\"Number of layers per token:\", len(token_embeddings[0]))\n",
    "#print(token_embeddings[0][0])\n",
    "#token_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features in a Token 768\n",
      "Number of Tokens 274\n"
     ]
    }
   ],
   "source": [
    "summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings] # [number_of_tokens, 768]\n",
    "n_columns = len(summed_last_4_layers[0]) #Number of Features in a Token\n",
    "n_rows = len(summed_last_4_layers) #Number of Tokens\n",
    "print(\"Number of Features in a Token\",n_columns)\n",
    "print(\"Number of Tokens\",n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84713763"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compare embedding of \"Computer\" at 2nd position and 81st position \n",
    "same_computer = cosine_similarity(summed_last_4_layers[1].reshape(1,-1), summed_last_4_layers[80].reshape(1,-1))[0][0]\n",
    "same_computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
